# -*- coding: utf-8 -*-
"""Untitled22 (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mh-BdqqrFiz3qz1Sr_a9ggJNqZ0bAqyL
"""

#import libary
import pandas as pd
import numpy as np
from zipfile import ZipFile
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

from google.colab import files
files.upload()

"""# Data Loading"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/kaggle.json
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d CooperUnion/anime-recommendations-database

!unzip anime-recommendations-database.zip

anime = pd.read_csv('anime.csv')
anime

rating = pd.read_csv('rating.csv')
rating

print("Baris data anime\t: ", anime.shape[0])
print("Kolom data anime\t: ", anime.shape[1], "\n")
print("Baris data user rating\t: ", rating.shape[0])
print("Kolom data user rating\t: ", rating.shape[1])

"""Pada data anime terdapat :
- Baris	:  12.294
- Kolom	:  7

Pada data rating terdapat :
- Baris	:  7.813.737
- Kolom	:  3
"""

anime.info()

rating.info()

anime.isnull().sum()

anime = anime.dropna(subset=['genre'])

anime.duplicated().sum()

rating.isnull().sum()

rating.duplicated().sum()

rating = rating.drop_duplicates()

all_genres = ','.join(anime['genre'].astype(str))

genre_list = all_genres.split(',')

unique_genres = sorted(set([g.strip() for g in genre_list]))

print(unique_genres)
print("Banyak genre : ", len(unique_genres))

"""pada data anime memiliki 43 jenis genre yang berbeda"""

anime['genre'] = anime['genre'].apply(lambda x: ' '.join(x) if isinstance(x, list) else (x.replace(',', ' ') if isinstance(x, str) else str(x)))

"""hapus tanda "," pada anime yang memiliki genre lebih dari 1

untuk memastikan representasi fitur genre yang benar saat menggunakan teknik Content-Based Filtering berbasis TF-IDF.
"""

all_anime_rate = rating
all_anime_rate

"""buat salinan data rating bernapa all_anime_rate"""

all_anime = pd.merge(all_anime_rate, anime[['anime_id','name','genre']], on='anime_id', how='left')

all_anime

"""mengambil data `name` dan `genre` dari data anime dan digabungkan menggunakan `anime_id`"""

all_anime["user_id"].value_counts()

"""mengecek seberapa banyak user melakukan rating. <br>
menemukan data dengan `user_id` = "48766" memiliki 10.227 rating
"""

all_anime[all_anime['user_id'] == 48766]["rating"].value_counts()

"""menemukan data dengan `user_id` = "48766" merating -1 semua. <br>

terdapat keanehan mengapa user tersebut merating 10.227 data -1 semua.
"""

all_anime = all_anime[all_anime['user_id'] != 48766]

"""karena tidak masuk akal, user tersebut akan dihapus"""

all_anime.isnull().sum()

"""terdapat data kosong pada kolom `nama` dan `genre`"""

all_anime_clean = all_anime.dropna()
all_anime_clean

"""karena tidak dapat diidentifikasi `nama` serta `genre`nya, maka diputuskan untuk menghapus data yang hilang tersebut"""

fix_anime = all_anime_clean.sort_values('anime_id', ascending=True)
fix_anime

len(fix_anime.anime_id.unique())

preparation = fix_anime
preparation.sort_values('anime_id')

preparation = preparation.drop_duplicates('anime_id')
preparation

"""Menghapus data `anime_id` yang duplikat agar data ini bisa digunakan untuk pemodelan"""

anime_id = preparation['anime_id'].tolist()

anime_name = preparation['name'].tolist()

anime_genre = preparation['genre'].tolist()

print(len(anime_id))
print(len(anime_name))
print(len(anime_genre))

"""konversi data menjadi list"""

anime_new = pd.DataFrame({
    'id': anime_id,
    'anime_name': anime_name,
    'genre': anime_genre
})
anime_new

"""membuat dictionary untuk menentukan pasangan key-value pada data anime_id, anime_name, dan genre yang telah disiapkan

# Modeling

## Content Based Filtering
"""

data = anime_new
data.sample(5)

"""assign dataframe yang sudah dibuat sebelumnya ke dalam variabel data"""

tf = TfidfVectorizer()

tf.fit(data['genre'])

tf.get_feature_names_out()

"""menemukan representasi fitur penting dari setiap genre anime."""

tfidf_matrix = tf.fit_transform(data['genre'])

tfidf_matrix.shape

"""fit dan transformasi ke dalam bentuk matriks"""

tfidf_matrix.todense()

"""Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()"""

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=data.anime_name
).sample(22, axis=1).sample(10, axis=0)

"""lihat matriks tf-idf untuk beberapa anime (anime_name) dan genre anime (genre)

Output matriks tf-idf di atas menunjukkan Anime Eat You Up/Bunny memiliki genre yaoi dan beberaoa anime menunjuk genrenya.
"""

cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

""" menghitung derajat kesamaan (similarity degree) antar restoran dengan teknik cosine similarity."""

cosine_sim_df = pd.DataFrame(cosine_sim, index=data['anime_name'], columns=data['name'])
print('Shape:', cosine_sim_df.shape)

cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""Melihat matriks kesamaan setiap anime dengan menampilkan nama anime dalam 5 sampel kolom dan 10 baris"""

def anime_recommendations(nama_anime, similarity_data=cosine_sim_df, items=data[['anime_name', 'genre']], k=5):

    index = similarity_data.loc[:,nama_anime].to_numpy().argpartition(
        range(-1, -k, -1))

    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    closest = closest.drop(nama_anime, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

"""membuat fungsi anime_recommendations dengan beberapa parameter sebagai berikut:

nama_anime : Nama anime (index kemiripan dataframe).
Similarity_data : Dataframe mengenai similarity yang telah kita definisikan sebelumnya.
Items : Nama dan fitur yang digunakan untuk mendefinisikan kemiripan, dalam hal ini adalah anime_name’ dan ‘genre.
k : Banyak rekomendasi yang ingin diberikan.
"""

anime[anime.name.eq('Naruto')]

anime_recommendations('One Punch Man')

"""mendapatkan anime recommendation dengan memanggil fungsi yang telah kita definisikan sebelumnya

# Collaboration Filtering
"""

rating = rating.sample(n=10_000, random_state=42)

df = rating
df

print("banyak data : ", df.shape[0])

"""Karena data sangat banyak sehingga sangat memakan waktu pelatihan, maka akan kami hapus random menjadi 10.000 data saja"""

user_ids = df['user_id'].unique().tolist()
print('list userID: ', user_ids)

user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

anime_ids = df['anime_id'].unique().tolist()

anime_to_anime_encoded = {x: i for i, x in enumerate(anime_ids)}

anime_encoded_to_anime = {i: x for i, x in enumerate(anime_ids)}

"""menyandikan (encode) fitur ‘user’ dan ‘anime_id’ ke dalam indeks integer."""

df['user'] = df['user_id'].map(user_to_user_encoded)

df['anime'] = df['anime_id'].map(anime_to_anime_encoded)

"""petakan user_id dan anime_id ke dataframe yang berkaitan."""

num_users = len(user_to_user_encoded)
print(num_users)

num_anime = len(anime_to_anime_encoded)
print(num_anime)

df['rating'] = df['rating'].values.astype(np.float32)

min_rating = min(df['rating'])

max_rating = max(df['rating'])

print('Number of User: {}, Number of Anime: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_anime, min_rating, max_rating
))

"""## Spliting Data untuk Training dan Validasi"""

df = df.sample(frac=1, random_state=42)
df

"""acak datanya terlebih dahulu agar distribusinya menjadi random."""

x = df[['user', 'anime']].values

y = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""bagi data train dan validasi dengan komposisi 80:20. Namun sebelumnya, kita perlu memetakan (mapping) data user dan anime menjadi satu value terlebih dahulu. Lalu, buatlah rating dalam skala 0 sampai 1 agar mudah dalam melakukan proses training.

## Training
"""

class RecommenderNet(tf.keras.Model):

  def __init__(self, num_users, num_anime, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_anime = num_anime
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding(
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1)
    self.anime_embedding = layers.Embedding(
        num_anime,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.anime_bias = layers.Embedding(num_anime, 1)

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0])
    user_bias = self.user_bias(inputs[:, 0])
    anime_vector = self.anime_embedding(inputs[:, 1])
    anime_bias = self.anime_bias(inputs[:, 1])

    dot_user_anime = tf.tensordot(user_vector, anime_vector, 2)

    x = dot_user_anime + user_bias + anime_bias

    return tf.nn.sigmoid(x)

"""Membuat model RecommendNet menggunakan keras"""

model = RecommenderNet(num_users, num_anime, 50)

model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.0001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""melakukan proses compile terhadap model.


"""

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val)
)

"""Melakukan training"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Model collaborative filtering yang digunakan menunjukkan kinerja yang baik karena berhasil menurunkan RMSE pada data training dan test secara konsisten. Grafik ini menjadi indikator bahwa proses pelatihan berjalan dengan benar dan model memiliki kemampuan generalisasi yang layak."""

anime_df = anime_new
df = rating

user_id = df.user_id.sample(1).iloc[0]
anime_visited_by_user = df[df.user_id == user_id]

anime_not_visited = anime_df[~anime_df['id'].isin(anime_visited_by_user.anime_id.values)]['id']
anime_not_visited = list(
    set(anime_not_visited)
    .intersection(set(anime_to_anime_encoded.keys()))
)

anime_not_visited = [[anime_to_anime_encoded.get(x)] for x in anime_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_anime_array = np.hstack(
    ([[user_encoder]] * len(anime_not_visited), anime_not_visited)
)

"""Mengambil sample"""

ratings = model.predict(user_anime_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_anime_ids = [
    anime_encoded_to_anime.get(anime_not_visited[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('anime with high ratings from user')
print('----' * 8)

top_anime_user = (
    anime_visited_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .anime_id.values
)

anime_df_rows = anime_df[anime_df['id'].isin(top_anime_user)]
for row in anime_df_rows.itertuples():
    print(row.anime_name, ':', row.genre)

print('----' * 8)
print('Top 10 anime recommendation')
print('----' * 8)

recommended_anime = anime_df[anime_df['id'].isin(recommended_anime_ids)]
for row in recommended_anime.itertuples():
    print(row.anime_name, ':', row.genre)

"""Mendapatkan hasil rekomendasi"""